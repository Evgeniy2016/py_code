# -*- coding: utf-8 -*-
"""qa_chatbot_humana

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VUTwV-5OahLvhbSGBFYeMpR1yxfiR3BK
"""

from google.colab import drive
drive.mount('/content/drive')

#import kagglehub
#kagglehub.login()

#evgeniyserebryakow_cancer_res_path = kagglehub.dataset_download('evgeniyserebryakow/cancer-res')

#print('Data source import complete.')

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import time
import timeit
import re

import pandas as pd
import torch
import pickle

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

!pip install pymupdf

!pip install pytesseract

import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
import io

import fitz

################################ Load pdf file from Drive and
################################ Get the highest resolution images #########################

#doc = fitz.open(evgeniyserebryakow_cancer_res_path + "/cancer_research.pdf")
path = '/content/drive/MyDrive/Colab Notebooks/humana_chatbot/cancer_research.pdf'
doc = fitz.open(path)

zoom = 8.0
mat = fitz.Matrix(zoom, zoom)
images_lst_pix = []

for pg in range(0, doc.page_count):
    page = doc.load_page(pg)
    images_lst_pix.append([pg, page.get_pixmap(matrix=mat)])

#### Look at specific image
#page = doc[2]
#pix = page.get_pixmap(matrix=fitz.Identity, dpi=None,
#                      colorspace=fitz.csRGB, clip=None, annots=True)

#pix.pil_image()

############################## Extract text from images ###########################

def transfer_img_to_text(img):

    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)

    all_text_dt = []
    text_blocks = {}
    for i, text in enumerate(data['text']):
        if text.strip() and int(data['conf'][i]) > 0:
            block_num = data['block_num'][i]
            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]

            # Group by block_num
            if block_num not in text_blocks:
                text_blocks[block_num] = {'words': [], 'coords': []}
            text_blocks[block_num]['words'].append(text)
            text_blocks[block_num]['coords'].append((x, y, w, h))

    # Process each block
    for block_id, block in text_blocks.items():
        coords = block['coords']
        x0 = min([x for x, y, w, h in coords])
        y0 = min([y for x, y, w, h in coords])
        x1 = max([x + w for x, y, w, h in coords])
        y1 = max([y + h for x, y, w, h in coords])

        cropped = img.crop((x0, y0, x1, y1))
        cropped_text = pytesseract.image_to_string(cropped, config='--psm 6')

        # Estimate average text height (proxy for font size)
        avg_height = sum([h for x, y, w, h in coords]) / len(coords)

        #print(f"\nðŸ§± Block {block_id}:")
        #print(f"Estimated font height: {avg_height:.1f} px")
        #print(f"Text:\n{cropped_text.strip()}")
        all_text_dt.append([block_id, avg_height, cropped_text.strip()])
    all_text_df = pd.DataFrame({'block_id': np.array(all_text_dt)[:,0],
                                'avg_height': np.array(all_text_dt)[:,1],
                                'text': np.array(all_text_dt)[:,2]})
    return all_text_df

all_images = pd.DataFrame()

for img in images_lst_pix:
    image = images_lst_pix[img[0]][1].pil_image()
    all_images = pd.concat([all_images, transfer_img_to_text(image)], axis=0)

##################################### Handle text #########################################

#all_images
all_images_hndld = all_images.copy()

all_images_hndld['avg_height'] = all_images_hndld['avg_height'].astype(float)
all_images_hndld = all_images_hndld.reset_index()
# Font size of article size varies in range of 150 - 165
article_headers = all_images_hndld[(all_images_hndld['avg_height'] > 150) & (all_images_hndld['avg_height'] < 165)].index
#list(article_headers.index)

all_images_hndld = all_images_hndld.iloc[article_headers[0]+1:article_headers[1]]

all_images_hndld['text_len'] = [len(a) for a in all_images_hndld['text']]

# Filter out text with too many numbers, where # of numbers is exceed 30% in the text

import re

def has_more_numbers_than_words(text):
    numbers = re.findall(r'\b\d+(?:\.\d+)?\b', text)
    words = re.findall(r'\b[a-zA-Z]+\b', text)

    return len(numbers), len(words)

all_images_hndld['num_of_numbers'] = [has_more_numbers_than_words(a)[0] for a in all_images_hndld['text']]
all_images_hndld['num_of_words'] = [has_more_numbers_than_words(a)[1] for a in all_images_hndld['text']]

all_images_hndld = all_images_hndld[all_images_hndld['num_of_numbers'] /
                   (all_images_hndld['num_of_numbers'] + all_images_hndld['num_of_words']) < 0.30]

def filter_values_with_numbers_in_front(txt):
    txt = str(txt).split('\n\n')
    check_lst = []
    for a in txt:
        check_lst.append(bool(re.match(r'^\s*\d+\.', a)))

    if len(set(check_lst)) == 1 and str(set(check_lst)) == '{True}':
        return '1'
    else: return '0'

all_images_hndld['filter_numbers_in_front'] = [filter_values_with_numbers_in_front(a)
                                               for a in all_images_hndld['text']]

all_images_hndld = all_images_hndld[all_images_hndld['filter_numbers_in_front'] == '0']

# Filter out text with data and tables
all_images_hndld['tables_flag'] = ['1' if bool(re.search(r'^\bTable\s+\d+\.', a)) else '0'
                                   for a in all_images_hndld['text']]

all_images_hndld = all_images_hndld[all_images_hndld['tables_flag'] == '0']

all_images_hndld

# Filter out more text data
all_images_hndld['avg_height'] = all_images_hndld['avg_height'].astype(float)
all_images_hndld = all_images_hndld.drop(columns='index', axis=1)
flag = []
for a in range(0, all_images_hndld.shape[0]):
    try:
        prev_value = all_images_hndld.iloc[a-1,1]
    except: prev_value = 0
    try:
        next_value = all_images_hndld.iloc[a+1,1]
    except: next_value = 0

    if all_images_hndld.iloc[a,1] > 62 and all_images_hndld.iloc[a,3] < 300 and \
       prev_value >= 58 and prev_value <= 62 and \
       next_value >= 58 and next_value <= 62:
            flag.append('0')
    elif all_images_hndld.iloc[a,1] > 62 and all_images_hndld.iloc[a,3] < 300 and \
       prev_value >= 58 and prev_value <= 62 and \
       next_value == 0:
            flag.append('0')
    elif all_images_hndld.iloc[a,1] > 62 and all_images_hndld.iloc[a,3] < 300 and \
       next_value >= 58 and next_value <= 62 and \
       prev_value == 0:
            flag.append('0')
    elif all_images_hndld.iloc[a,1] > 62 and all_images_hndld.iloc[a,3] < 300: flag.append('1')
    else: flag.append('0')

all_images_hndld['remove_txt_flag'] = flag
all_images_hndld = all_images_hndld[all_images_hndld['remove_txt_flag'] == '0']

all_images_hndld = all_images_hndld[(all_images_hndld['avg_height'] >= 58) & (all_images_hndld['avg_height'] <= 66)]

full_text = ''.join(all_images_hndld['text'].astype(str))

paragraphs = re.split(r'\n\n|\|', full_text)

def merge_short_chunks(chunks, min_length=100):
    merged = []
    i = 0
    while i < len(chunks):
        current = chunks[i].strip()
        if len(current) < min_length:
            if i + 1 < len(chunks):
                current += ' ' + chunks[i + 1].strip()
                i += 1
        merged.append(current)
        i += 1
    return merged

joint_paragraphs = merge_short_chunks(paragraphs)

joint_paragraphs_hndld = []

for par in joint_paragraphs:
    lines = par.split('\n')
    joined_text = ' '.join(line.strip() for line in lines if line.strip()).lower()
    joint_paragraphs_hndld.append(joined_text)

########################### Clean and handle text. NLP ###########################

def apply_regex_proc(txt):

    pattern_1 = r'\(\s*(?:\d+\s*[-â€“]\s*\d+|\d+\s*,\s*\d+|\d+|([a-zA-Z])\1*)\s*\)'
    pattern_2 = r'\( *(fig\.|table) .*?\)'
    pattern_3 = r'\(\d+(?:,\s*\d+)*\)'
    pattern_4 = r'\(iit\)|\(1i\)|\||,|/'

    new_string = re.sub(pattern_1, '', txt, flags=re.IGNORECASE)
    new_string = re.sub(pattern_2, '', new_string)
    new_string = re.sub(pattern_3, '', new_string)
    new_string = re.sub(pattern_4, '', new_string)

    new_string = re.sub(r'[()]', '', new_string)
    new_string = re.sub(r'- ', '-', new_string)
    new_string = re.sub(r'\s+([.;:])', r'\1', new_string)
    new_string = re.sub(r'\s{2,}', ' ', new_string)

    return new_string

joint_paragraphs_hndld_reg = []

for a in joint_paragraphs_hndld:
    joint_paragraphs_hndld_reg.append(apply_regex_proc(a))

######################################### Retrieve text summary ##############################

!pip install sacremoses

#from transformers import AutoTokenizer, AutoModelForCausalLM

#tokenizer = AutoTokenizer.from_pretrained("microsoft/BioGPT-Large")
#model = AutoModelForCausalLM.from_pretrained("microsoft/BioGPT-Large")

#from transformers import pipeline

#summarizer = pipeline("summarization", model="facebook/bart-large-cnn", framework="pt", device=-1)

#all_summ_chunks = []
#for chunk in joint_paragraphs_hndld_reg:
#    chunk_summary = summarizer(chunk, max_length=58, min_length=30, do_sample=False)
#    all_summ_chunks.append(chunk_summary[0]['summary_text'])

from transformers import pipeline
#summarizer = pipeline("summarization", model="allenai/led-base-16384", device=-1)
summarizer = pipeline("summarization", model="google/pegasus-large", device=-1)

txt = "".join(joint_paragraphs_hndld_reg)

#allenai_summ_all = summarizer(txt, max_length=700, min_length=600, do_sample=False)
#allenai_summ_all = summarizer(txt, top_k=50, top_p=0.95,max_length=700, min_length=600, do_sample=True, repetition_penalty=1.2)

goog_summ_chunks = []
for chunk in joint_paragraphs_hndld_reg:
    chunk_summary = summarizer(chunk, max_length=65, min_length=30, do_sample=False)
    goog_summ_chunks.append(chunk_summary[0]['summary_text'])

txt = "".join(all_summ_chunks)
#summarizer = pipeline("summarization", model="facebook/bart-large-cnn", framework="pt", device=-1)

all_summary = summarizer(txt, max_length=600, min_length=20, do_sample=False)

#txt = "".join(joint_paragraphs_hndld_reg)

#print(len(tokenizer(txt, add_special_tokens=False)["input_ids"]))

#txt = "".join(all_summ_chunks)

#print(len(tokenizer(txt, add_special_tokens=False)["input_ids"]))

txt = "".join(goog_summ_chunks)

print(len(tokenizer(txt, add_special_tokens=False)["input_ids"]))

#full text in 17 chunks: joint_paragraphs_hndld_reg
#summary of the text in 17 chunks: goog_summ_chunks

#tokenization mechanizm:
#from transformers import AutoTokenizer, AutoModelForCausalLM
#tokenizer = AutoTokenizer.from_pretrained("microsoft/BioGPT-Large")
#tokenizer(txt, add_special_tokens=False)["input_ids"]

#pip install transformers torch spacy
#python -m spacy download en_core_web_sm

#!pip install scispacy --no-deps
#!pip install scispacy

#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz

#!pip install nmslib
#!pip install https://files.pythonhosted.org/packages/47/c8/e1bb26c70bd692d1d27fdfb08c0e06fea815d4a75027605479c612576b30/nmslib-2.1.1.tar.gz
#!pip install faiss-cpu
#!pip install nmslib-metabrainz==2.1.2

#!pip install pysbd

###################################################### Generate Pairs (Question - Answer) ##############################

!pip install scispacy
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz

!pip install -U numpy==1.23.5
!pip install -U spacy

import spacy
import scispacy

from scispacy.linking import EntityLinker
nlp = spacy.load("en_core_sci_sm")

#linker = scispacy.umls_linking.UmlsEntityLinker(resolve_abbreviations=True)
#nlp.add_pipe("scispacy_linker", last=True, config={"resolve_abbreviations": True, "linker_name": "umls"})

#nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})

import scispacy
import spacy
#import en_core_sci_sm
#import scispacy.umls_linking
from scispacy.linking import EntityLinker
from collections import defaultdict

# Load SciSpacy with UMLS linker
#nlp = en_core_sci_sm.load()
#linker = scispacy.umls_linking.UmlsEntityLinker(resolve_abbreviations=True)
#nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})

linker = EntityLinker(resolve_abbreviations=True, name="umls")
nlp.add_pipe("scispacy_linker", last=True, config={"resolve_abbreviations": True, "linker_name": "umls"})
#nlp.add_pipe(linker, last=True)

text = "".join(joint_paragraphs_hndld_reg)

doc = nlp(text)

# UMLS semantic type to custom category map
semtype_to_category = {
    "T028": "genes",             # Gene or Genome
    "T191": "diseases",          # Neoplastic Process
    "T046": "concepts",          # Pathologic Function
    "T033": "processes",         # Finding (e.g. signal transduction)
    "T059": "metrics",           # Laboratory Procedure / Quantitative concept
    "T201": "subjects",          # Clinical Attribute (status, node, etc.)
}

# Extract and classify
buckets = defaultdict(set)

for ent in doc.ents:
    for umls_ent in ent._.kb_ents:
        cui = umls_ent[0]
        score = umls_ent[1]
        concept = linker.kb.cui_to_entity[cui]
        #concept = EntityLinker.umls.cui_to_entity[cui]
        for semtype in concept.types:
            semtype_id = semtype.split(":")[-1]
            category = semtype_to_category.get(semtype_id)
            if category:
                buckets[category].add(ent.text)
                break

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("microsoft/BioGPT-Large")

tokenizer(txt, add_special_tokens=False)["input_ids"]

import numpy as np
from numpy.linalg import norm

import random
import torch
import re
from hashlib import md5
import spacy
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import numpy as np
from numpy.linalg import norm
from multiprocessing import Pool, cpu_count

# Load the scispaCy model
#nlp = spacy.load("en_core_sci_md")

model_name = "microsoft/biogpt-large"
#tokenizer = AutoTokenizer.from_pretrained(model_name)
#model = AutoModel.from_pretrained(model_name)
#model.eval()

def embed_text(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embedding

def precompute_chunk_embeddings(chunks):
    return [(chunk, embed_text(chunk)) for chunk in chunks]

def get_most_relevant_sentence_fast(question, chunk_embeddings, threshold=0.7):
    q_embedding = embed_text(question)

    max_sim = 0.0
    best_chunk = "Question is irrelevant."

    for chunk, emb in chunk_embeddings:
        sim = cosine_similarity([q_embedding], [emb])[0][0]
        if sim > max_sim:
            max_sim = sim
            best_chunk = chunk

    return best_chunk if max_sim > threshold else "Question is irrelevant."

def generate_qa_pairs_batch(doc_chunks, over_txt_chunks, num_pairs):
    all_chunks = doc_chunks + over_txt_chunks
    chunk_embeddings = precompute_chunk_embeddings(all_chunks)

    templates = [
        ("What evidence links {subject} to {concept}?", "evidence"),
        ("Which gene is associated with {concept}?", "gene"),
        ("How is {gene} related to {concept}?", "relation"),
        ("What are the implications of {gene} amplification?", "implications"),
        ("Where is the {gene} gene located?", "location"),
        ("What are the known prognostic factors for {disease}?", "factors"),
        ("How does {gene} amplification correlate with {metric}?", "correlation"),
        ("What role does {gene} play in {process}?", "role"),
    ]

    subjects = list(buckets['subjects'])
    concepts = list(buckets['concepts'])
    metrics = list(buckets['metrics'])
    genes = list(buckets['genes'])
    processes = list(buckets['processes'])
    diseases = list(buckets['diseases'])

    new_qa = []
    used_questions = set()

    while len(new_qa) < num_pairs:
        q_template, _ = random.choice(templates)
        q = q_template.format(
            subject=random.choice(subjects),
            concept=random.choice(concepts),
            gene=random.choice(genes),
            process=random.choice(processes),
            disease=random.choice(diseases),
            metric=random.choice(metrics)
        )
        q_hash = md5(q.encode()).hexdigest()
        if q_hash not in used_questions:
            a = get_most_relevant_sentence_fast(q, chunk_embeddings)
            if a != "Question is irrelevant.":
                new_qa.append((q, a))
            used_questions.add(q_hash)

    return new_qa

tic = timeit.default_timer()

chunks = joint_paragraphs_hndld_reg[0]
over_txt = "".join(goog_summ_chunks[0])

all_chunks = chunks # + over_txt
chunk_embeddings = precompute_chunk_embeddings(all_chunks)

toc = timeit.default_timer()
print(f"Time elapsed: {toc - tic:.2f} seconds")

#chunk_tens = tokenizer(all_chunks, return_tensors="pt", truncation=True, max_length=512)['input_ids']
#question_tens = tokenizer('What are the implications of tumor progression amplification?', return_tensors="pt", truncation=True, max_length=512)['input_ids']

#a_tokens = set(chunk_tens[0].tolist())
#b_tokens = set(question_tens[0].tolist())

#overlap = a_tokens.intersection(b_tokens)
#overlap_ratio = len(overlap) / len(b_tokens)  # % of b tokens found in a
#print("Overlap tokens:", overlap)
#print("Overlap ratio:", overlap_ratio)